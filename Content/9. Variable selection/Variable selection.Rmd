---
title: "Variable selection"
author: "Perry de Valpine"
date: "September 2019"
output:
  slidy_presentation: default
  beamer_presentation: default
---
<style>
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE,
                      dev = 'png') # Reduce size from plots with large samples

library(nimble)
library(compareMCMCs)
calculate <- TRUE
```

Bayesian variable selection
=====

- You have many candidate explanatory variables.
- Bayesian approach is to have a probability that a variable is included in the model.
- Really this is a probability that the coefficient is $\ne 0$.
- BUGS/JAGS implementation is with indicator variables.
- This has problems.  Let's look at it.

Set up a model
=====
Use linear regression for simplicity.

- 5 "real" effects (true coefficient $\ne 0$)
- 10 null effects  (true coefficient $= 0$).

### Create `nimble` model
```{r}
lmCode <- nimbleCode({
  psi ~ dunif(0,1)   # prior on inclusion probability
  sigma ~ dunif(0, 20)
  for(i in 1:numVars) {
    z[i] ~ dbern(psi) # indicator variable
    beta[i] ~ dnorm(0, sd = 100)
    zbeta[i] <- z[i] * beta[i]  # indicator * beta
  }
  for(i in 1:n) {
    pred.y[i] <- inprod(X[i, 1:numVars], zbeta[1:numVars])
    y[i] ~ dnorm(pred.y[i], sd = sigma)
  }
})
set.seed(1)
X <- matrix(rnorm(100*15), nrow = 100, ncol = 15)
lmConstants <- list(numVars = 15, n = 100, X = X)
lmModel <- nimbleModel(lmCode, constants = lmConstants)
```

### Simulate data using `nimble` model.
```{r}
true_betas <- c(c(0.1, 0.2, 0.3, 0.4, 0.5),
                rep(0, 10))
lmModel$beta <- true_betas
lmModel$sigma <- 1
lmModel$z <- rep(1, 15)
lmModel$psi <- 0.5
lmModel$calculate()
set.seed(0) ## Make this reproducible
lmModel$simulate('y')
lmModel$y
lmModel$calculate() 
lmModel$setData('y')
lmData = list(y = lmModel$y)
```

Look at `lm`
=====
It will be helpful to refer to back simple linear regression:
```{r}
summary(lm(lmModel$y ~ lmModel$X))
```

Look at JAGS
=====

### Rewrite model to be JAGS-compatible
```{r eval=calculate}
## Switch from normal with sd to normal with precision:
lmCodeJAGS <- nimbleCode({
  psi ~ dunif(0,1)   # prior on inclusion probability
  sigma ~ dunif(0, 20)
  tau <- 1/(sigma*sigma)
  for(i in 1:numVars) {
    z[i] ~ dbern(psi) # indicator variable
    beta[i] ~ dnorm(0, 1e-4)
    zbeta[i] <- z[i] * beta[i]  # indicator * beta
  }
  for(i in 1:n) {
    pred.y[i] <- inprod(X[i, 1:numVars], zbeta[1:numVars])
    y[i] ~ dnorm(pred.y[i], tau)
  }
})
```

### Run model through compareMCMCs as a way to run JAGS from nimble.
```{r eval=calculate}
result_jags <- compareMCMCs(
  seed = 100, ## make it reproducible
  list(code = lmCodeJAGS,
       constants = c(lmConstants,
                     lmData),
       inits = list(psi = 0.5,
                    beta = true_betas,
                    z = rep(1, 15))),
  MCMCs = "jags",
  monitors = c(lmModel$getNodeNames(topOnly = TRUE), "z"),
  MCMCcontrol = list(niter = 100000, burnin = 10000)
)
samples_jags <- result_jags$jags$samples
cat("JAGS run time was:")
print(result_jags$jags$times$sample)
```

Look at JAGS results
=====

### Look at beta[1]
```{r}
inds <- 50001:60000 ## Look at arbitrary 10000 iterations
plot(samples_jags[inds,'beta[1]'], pch = '.')
plot(samples_jags[inds,'z[1]'], pch = '.')
```

### Look at beta[4]
```{r}
plot(samples_jags[inds,'beta[4]'], pch = '.')
plot(samples_jags[inds,'z[4]'], pch = '.')
```

### Look at beta[5]
```{r}
plot(samples_jags[inds,'beta[5]'], pch = '.')
plot(samples_jags[inds,'z[5]'], pch = '.')
```

### Look at posterior inclusion probabilities for each `beta[i]`
```{r}
zCols <- grep("z\\[", colnames(samples_jags))
posterior_inclusion_prob_jags <- colMeans(samples_jags[,zCols])
plot(true_betas, posterior_inclusion_prob_jags)
```

### Look at posterior inclusion probability, `psi`
```{r}
plot(density(samples_jags[,'psi']))
```

Summary of JAGS results
=====

- Marginal posterior inclusion probabilities look reasonable.
- Mixing over `z[i]`s is slow.  For shorter runs, we would not be happy with mixing.
- When `z[i]` is 0, `beta[i]`s follow their priors, until they hit values that allow `z[i]` of 1 to be accepted.
- JAGS uses slice samplers.
- I think a common work-around is to use informative priors.  That means changing your model to help the MCMC implementation.

Look at nimble results with default samplers.
=====
```{r}
MCMCconf <- configureMCMC(lmModel)
MCMCconf$addMonitors('z')
MCMC <- buildMCMC(MCMCconf)
ClmModel <- compileNimble(lmModel)
CMCMC <- compileNimble(MCMC, project = lmModel, resetFunctions = TRUE)
set.seed(100)
system.time(samples_nimble <- runMCMC(CMCMC, niter = 100000, nburnin = 10000))
```


Look at default nimble results
=====

### Look at beta[1]
```{r}
inds <- 50001:60000 ## Look at arbitrary 10000 iterations
plot(samples_nimble[inds,'beta[1]'], pch = '.')
plot(samples_nimble[inds,'z[1]'], pch = '.')
```

### Look at beta[4]
```{r}
plot(samples_nimble[inds,'beta[4]'], pch = '.', ylim = c(-1, 1))
plot(samples_nimble[inds,'z[4]'], pch = '.')
```

### Look at beta[5]
```{r}
plot(samples_nimble[inds,'beta[5]'], pch = '.')
plot(samples_nimble[inds,'z[5]'], pch = '.')
```

### Look at posterior inclusion probabilities from each `beta[i]`
```{r}
zCols <- grep("z\\[", colnames(samples_nimble))
posterior_inclusion_prob_nimble <- colMeans(samples_nimble[,zCols])
plot(true_betas, posterior_inclusion_prob_nimble)
```

### Look at posterior inclusion probability, `psi`
```{r}
plot(density(samples_nimble[,'psi']))
```

Summary of results from default nimble
=====

- Different from (probably worse that) JAGS.
- `beta[3]`, `beta[4]` and `beta[5]` are included almost always (posterior probability near 1).
- When `z[i] = 0`, the corresponding `beta[i]` will start following its **prior**.  It may wander far away from reasonable values, with either fast or slow mixing (depending on the situation).


Summary of JAGS and default nimble
=====

### Mixing problems (the main issue)
- The model doesn't understand the role of `z[i]`.
- When `z[i] = 0`, the corresponding `beta[i]` will start following its **prior**.
- A proposal to set `z[i] = 1` can only be accepted if `beta[i]` has a reasonable value.
- This creates poor mixing over `z[i]`s.

     - With a slice sampler (JAGS) `beta[i]` mixes well over its prior, and `z[i]` doesn't get set to 0 unless `beta[i]` happens to hit a small range of values to be included in the model.
     - With a random-walk MH sampler (default nimble), adaptation depends on `z[i]` (is it adapting to prior or posterior?), and this affects mixing when `z[i]` is 0.  Behavior seems to be problematic in this example.
     
- JAGS use of slice samplers seems to lead to reasonable but slow-mixing results in this example.

- `nimble` use of RW MH samplers seems to give problematic results.
     
- Presumably `nimble` with slice samplers on `beta[i]`s would perform similarly to JAGS.     
     
- Conventional solution in BUGS/JAGS language: Use informative prior for `beta[i]` to avoid these problems.  This is changing the model because the MCMC implementation has a problem.

### Wasteful computation (a secondary issue)
- When `z[i] = 0`, we'd like to not be wasting computation on `beta[i]`.

Solution: Reversible Jump MCMC
=====

- RJMCMC is a method for sampling across different models.
- Specifically it is about sampling between different numbers of dimensions.
- We don't change the actual nimble model object, but we turn on and off which dimensions are sampled.
- Implementation, like all samplers, is written using `nimbleFunction`s.

RJMCMC for variable selection in nimble
=====

- Update an MCMC configuration to use RJMCMC.

```{r}
# make a new copy of the model to be totally independent
lmModel2 <- lmModel$newModel(replicate = TRUE)
MCMCconfRJ <- configureMCMC(lmModel2)
MCMCconfRJ$addMonitors('z')
configureRJ(MCMCconfRJ,
            targetNodes = 'beta',
            indicatorNodes = 'z',
            control = list(mean = 0, scale = .2))
MCMCRJ <- buildMCMC(MCMCconfRJ)
```

Run the RJMCMC
=====
```{r}
ClmModel2 <- compileNimble(lmModel2)
CMCMCRJ <- compileNimble(MCMCRJ, project = lmModel2)
set.seed(100)
system.time(samples_nimble_RJ <- runMCMC(CMCMCRJ, niter = 100000, nburnin = 10000))
```

Look at RJMCMC results
=====

### Look at beta[1] 
```{r}
inds <- 50001:60000
plot(samples_nimble_RJ[inds,'beta[1]'], pch = '.')
plot(samples_nimble_RJ[inds,'z[1]'], pch = '.')
```

### Look at beta[4]
```{r}
plot(samples_nimble_RJ[inds,'beta[4]'], pch = '.')
plot(samples_nimble_RJ[inds,'z[4]'], pch = '.')
```

### Look at beta[5]
```{r}
plot(samples_nimble_RJ[inds,'beta[5]'], pch = '.')
plot(samples_nimble_RJ[inds,'z[5]'], pch = '.')
```

### Look at posterior inclusion probabilities of each `beta[i]`
```{r}
zCols <- grep("z\\[", colnames(samples_nimble_RJ))
posterior_inclusion_prob_nimble_RJ <- colMeans(samples_nimble_RJ[,zCols])
plot(true_betas, posterior_inclusion_prob_nimble_RJ)
```

### Look at posterior inclusion probability, `psi`
```{r}
plot(density(samples_nimble_RJ[,'psi']))
```

Summary of RJMCMC
=====

- Mixing was much better.
- Result looks intuitively more sensible.
- Adaptation for coefficient samplers only occurs when the coefficient is "in the model".
- Run time was much faster than default nimble, which was faster than JAGS.  Magnitudes will depend on specific problems (how often `z[i]` are 0.)
- Tuning parameter of RJ proposal scale (sd) must be chosen.
